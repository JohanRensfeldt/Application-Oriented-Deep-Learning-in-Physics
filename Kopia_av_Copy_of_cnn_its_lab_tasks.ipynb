{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohanRensfeldt/Application-Oriented-Deep-Learning-in-Physics/blob/main/Kopia_av_Copy_of_cnn_its_lab_tasks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XblK19KM9VKF"
      },
      "source": [
        "## DNA motif discovery using CNN\n",
        "\n",
        "In this lab, we will use convolutional neural networks (CNN) to detect certain motifs (DNA fragments following specific pattern) in plant DNA sequences downloaded from GenBank.\n",
        "\n",
        "### Data\n",
        "The first step is to go to [GenBank](https://https.ncbi.nlm.nih.gov/genbank/) database [FTP service](https://ftp.ncbi.nih.gov/genbank/) and download some data. Whenever researchers sequence DNA coming from an organism and publish their finding, the DNA sequences are deposited in GeneBank. In their FTP service, one can see a number of `gb***N.seq.gz` files where `***` is an abbreviation for a group of organisms DNA sequences come from, e.g. `bct` for bacteria or `pln` for plants and `N` is a sequence number assigned in a consecutive manner to deposited files. We have downloaded one of such files `gbpln132.seq.gz` and converted it to an easier-to-read format called [FASTA](https://en.wikipedia.org/wiki/FASTA). If you are curious and would like to prepare your own dataset, we used the following `BioPython` file IO routines to convert from GenBank format to FASTA:\n",
        "```\n",
        "from Bio import SeqIO\n",
        "SeqIO.convert(\"gbpln132.seq\", \"genbank\", \"gbpln132.fasta\", \"fasta\")\n",
        "```\n",
        "\n",
        "### DNA metabarcoding and amplicon sequencing\n",
        "\n",
        "Now, let me introduce you to a real research problem. Say, we go to a grocery store and buy a mix of spices. There are some spices listed on a label, but how can we make sure that there actually is basil or onion in the mix? One of the ways to figure this out is to extract DNA from these dried powdered spices and use it to single out species composition. This part can be somewhat tricky itself as it is not trivial to extract DNA from such mix that possibly also contains some chemicals that may damage the DNA or make extraction very inefficient. Say, however, we have solved this issue, now we can go two ways: \n",
        "* sequence all DNA fragments and hope they will say something about the species composition. The risk is, however, that we won't have enough material that is specific to a given species and thus, we end up below the detection level.\n",
        "* try to PCR-amplify (make many copies) of some 'magic' sequence fragments that, we know, are very specific/characteristic to different species. This is called **amplicon sequencing**. \n",
        "\n",
        "In this lab, we will go for the second approach. How does it work? Well, first, we need to have some barcodes (special DNA sequence fragments) that are suitable for species identification.\n",
        "\n",
        "### Properties a DNA barcode\n",
        "\n",
        "Back to our jar of spices -- we know that, at least in theory, the plants that are on the label should be there, but what else can this jar contain? It can be anything! In fact, any imaginable plant species in the world! So, what we need is a DNA fragment that we know all plants have, so that we can amplify this sequence fragment and we do not risk missing out something. At the same time this should be a **barcode**, i.e. a sequence that is different between all plant species so that we can uniquely identify its source. Sounds like a contradiction? What about a sequence that looks something like this:  \n",
        "\n",
        "```\n",
        "cccccccccccccccccUUUUUUUUUUUUUUUUUUCCCCCCCCCCCCCC\n",
        "```\n",
        "where `cccccccccc` and `CCCCCCCCCCC` are common for all plants and `UUUUUUUUU` fragment inside is unique for every plant species. What we can do now is to design molecular probes that will fish out `UUUUUUUU` based on it being flanked by `cccc` and `CCCCCC`. \n",
        "\n",
        "### ITS1 and ITS2\n",
        "\n",
        "Quite fortunately for us, such barcodes exist and the two most commonly used types of sequences used for plan identification are called ITS1 and ITS2. They look exactly as we want them to look. Well, with a caveat that sometimes the `UUUUUU` part is similar between some plants but the ITS-es are good enough for many applications. You can read more about ITS-es [here](https://en.wikipedia.org/wiki/Internal_transcribed_spacer).\n",
        "\n",
        "### Our task\n",
        "In this lab, we will see whether we can train CNN to be able to detect short sequence fragments (k-mers) coming from ITS and non-ITS sequences. If we succeed, we will have made the first step towards species identification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBBoquco9UFZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2d16893-60cf-4323-de84-6847009e13b2"
      },
      "source": [
        "!pip install biopython\n",
        "import os\n",
        "from Bio import SeqIO\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Download example GenBank data\n",
        "!wget https://www.dropbox.com/s/zjze886uove04s1/gbpln132.fasta\n",
        "\n",
        "# Read fasta file with sequences\n",
        "input_file = open('gbpln132.fasta', 'r')\n",
        "\n",
        "k_mer_size = 100 # we set our k-mer size here\n",
        "frac_to_take = 0.01 # fraction of sequences to work with due to memeory limits of Colab\n",
        "\n",
        "seq_lengths = []\n",
        "its_seq = []\n",
        "non_its_seq = []\n",
        "\n",
        "# Below we filter out sequences shorter than k-mer size and the ones \n",
        "# that contain Ns (we can change it later). We also populate its1, its2 and non-its\n",
        "# arrays. \n",
        "for sequence in SeqIO.parse(input_file, \"fasta\") :\n",
        "  seq_lengths.append(len(sequence.seq))\n",
        "\n",
        "  ##############################################################################\n",
        "  # TASK 1: complete the code so that all sequences that:\n",
        "  # - contain 'ITS' in sequence description and does not contain 2 or  \n",
        "  # more consecutive N (missing data) are added to its_seq\n",
        "  # - does not contain 'ITS' in the description and have a stretch of max 1 N \n",
        "  # are added to non_its\n",
        "  # ... \n",
        "  ##############################################################################\n",
        "\n",
        "input_file.close()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.79-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from biopython) (1.21.6)\n",
            "Installing collected packages: biopython\n",
            "Successfully installed biopython-1.79\n",
            "--2022-10-13 18:56:10--  https://www.dropbox.com/s/zjze886uove04s1/gbpln132.fasta\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/zjze886uove04s1/gbpln132.fasta [following]\n",
            "--2022-10-13 18:56:11--  https://www.dropbox.com/s/raw/zjze886uove04s1/gbpln132.fasta\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc30fc256048ad10f503bdd49e10.dl.dropboxusercontent.com/cd/0/inline/BuwpC5Q2OWEv4eknhQeqqPtg37BBtR-ui639PLd_XmS0BVxA7Jn04h4b3Z7JOsVDsrK3mVuEF3vNabOapjsruXBp3X-wNgldx9JnyuadSOSBO8TlYpim-Wybrb7LhEqb2Ks66-O6DWDdyNKH9mgb_wShp8OtnNIdcksyvMEkCWXP4Q/file# [following]\n",
            "--2022-10-13 18:56:11--  https://uc30fc256048ad10f503bdd49e10.dl.dropboxusercontent.com/cd/0/inline/BuwpC5Q2OWEv4eknhQeqqPtg37BBtR-ui639PLd_XmS0BVxA7Jn04h4b3Z7JOsVDsrK3mVuEF3vNabOapjsruXBp3X-wNgldx9JnyuadSOSBO8TlYpim-Wybrb7LhEqb2Ks66-O6DWDdyNKH9mgb_wShp8OtnNIdcksyvMEkCWXP4Q/file\n",
            "Resolving uc30fc256048ad10f503bdd49e10.dl.dropboxusercontent.com (uc30fc256048ad10f503bdd49e10.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uc30fc256048ad10f503bdd49e10.dl.dropboxusercontent.com (uc30fc256048ad10f503bdd49e10.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 115134086 (110M) [text/plain]\n",
            "Saving to: ‘gbpln132.fasta’\n",
            "\n",
            "gbpln132.fasta      100%[===================>] 109.80M  61.7MB/s    in 1.8s    \n",
            "\n",
            "2022-10-13 18:56:13 (61.7 MB/s) - ‘gbpln132.fasta’ saved [115134086/115134086]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZogP67p1j0zU"
      },
      "source": [
        "# we quickly check how many sequences do we have in every class\n",
        "print(len(its_seq), len(non_its_seq))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jNtDN7348XF"
      },
      "source": [
        "\n",
        "# Randomly select N non-its sequences to match the number of its1+its2 sequences\n",
        "import random\n",
        "its_len = len(its_seq)\n",
        "non_its_seq = random.sample(non_its_seq, its_len)\n",
        "print(\"ITS:\", len(its_seq))\n",
        "print(\"non-ITS:\", len(non_its_seq))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAojcJ_WKomF"
      },
      "source": [
        "# Here, we will plot sequence length distribution\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20,15))\n",
        "sns.histplot(data = np.log10(seq_lengths), bins = 250)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye9_t9D4tsNA"
      },
      "source": [
        "def getKmers(sequence, size):\n",
        "\n",
        "    ############################################################################\n",
        "    # TASK 2: write the body of the function that extracts all k-mers of a \n",
        "    # given length (size) from a sequence. E.g. for a sequence SEQUENCE and \n",
        "    # size 3: {SEQ, EQU, QUE, UEN, ENC, NCE}.\n",
        "    ############################################################################\n",
        "\n",
        "# Given a sequence, get all possible k-mers of length l along it.\n",
        "its = []\n",
        "non_its = []\n",
        "for i in its_seq:\n",
        "  its.extend(getKmers(i, k_mer_size))\n",
        "  \n",
        "for i in non_its_seq:\n",
        "  non_its.extend(getKmers(i, k_mer_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuMXG09m2B2k"
      },
      "source": [
        "print(\"Length of ITS:\", len(its))\n",
        "print(\"Length of non-ITS:\", len(non_its))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7DBGRqSByqH"
      },
      "source": [
        "# Randomly pick up a subset of sequences \n",
        "import math\n",
        "its = random.sample(its, math.floor(len(its) * frac_to_take))\n",
        "non_its = random.sample(non_its, math.floor(len(non_its) * frac_to_take))\n",
        "print(\"Length of ITS:\", len(its))\n",
        "print(\"Length of non-ITS:\", len(non_its))\n",
        "print(\"First 2 k-mers in a class:\")\n",
        "print(\"ITS: \", its[0:2])\n",
        "print(\"non-ITS: \", non_its[0:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiLWsCM7RFYF"
      },
      "source": [
        "# Encode bases in our k-mers so that each of the 4 channels corresponds to \n",
        "# the probability of each of the four bases.\n",
        "\n",
        "def encodeSeq(seq):\n",
        "  dict = {'A':[1,0,0,0],\n",
        "            'T':[0,1,0,0],\n",
        "            'C':[0,0,1,0],\n",
        "            'G':[0,0,0,1],\n",
        "            'R':[.5,0,0,.5],\t         # A or G\n",
        "            'Y':[0,.5,.5,0],\t         # C or T\n",
        "            'S':[0,0,.5,.5],        \t # G or C\n",
        "            'W':[.5,.5,0,0],           # A or T\n",
        "            'K':[0,.5,0,.5],       \t   # G or T\n",
        "            'M':[.5,0,.5,0],\t         # A or C\n",
        "            'B':[0,.33,.33,.33],\t     # C or G or T\n",
        "            'D':[.33,.33,0,.33],\t     # A or G or T\n",
        "            'H':[.33,.33,.33,0],\t     # A or C or T\n",
        "            'V':[.33,0,.33,.33],\t     # A or C or G\n",
        "            'N':[.25,.25,.25,.25]      # A or T or C or G\n",
        "          }\n",
        "  seq = list(seq.upper())\n",
        "  encoded = [dict[base] for base in seq]\n",
        "  return encoded\n",
        "\n",
        "################################################################################\n",
        "# TASK 3: create lists of encoded both its and the non-its sequences  \n",
        "# using the provided encodeSeq function. \n",
        "################################################################################\n",
        "\n",
        "encoded_its = ...\n",
        "encoded_non_its = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYFQsE88SdEj"
      },
      "source": [
        "print(\"First 2 encoded k-mers in a class:\")\n",
        "print(\"Lenghts of encoded ITS: \", len(encoded_its),  \", non-its: \", len(encoded_non_its))\n",
        "print(encoded_its[0:2])\n",
        "#print(encoded_its2[0:2])\n",
        "print(encoded_non_its[0:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R7txzEgfJoB"
      },
      "source": [
        "# Here, we will one-hot encode labels\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "labels = list(np.ones(len(encoded_its))) + list(np.zeros(len(encoded_non_its)))\n",
        "\n",
        "# One-hot encode Sequences\n",
        "#integer_encoder = LabelEncoder()  \n",
        "one_hot_encoder = OneHotEncoder()  \n",
        "labels = np.array(labels).reshape(-1, 1)\n",
        "input_labels = one_hot_encoder.fit_transform(labels).toarray()\n",
        "print('Labels:\\n', labels.T) \n",
        "print('One-hot encoded labels:\\n', input_labels.T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_UBx9oHhg2e"
      },
      "source": [
        "# Prepare the training set\n",
        "input_features = np.array(encoded_its + encoded_non_its)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_oZDpnvPAQY"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(input_features, input_labels, test_size = 0.25, random_state = 42)\n",
        "print(train_features[0])\n",
        "print(train_features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozutvEDshwmn"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam, Adadelta, SGD\n",
        "from keras.layers import Conv1D, Dense, MaxPooling1D, Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.regularizers import l2, l1\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "################################################################################\n",
        "# TASK 4: define a sequential model with the following layers \n",
        "# (use l2 regularizer when possible):\n",
        "# - convolutional 1D input layer (64 filters, kernel size 20, padding set to 'same' and 'relu' activation)\n",
        "# - convolutional 1D with less filters and smaller kernel \n",
        "# - max pooling layer with pool size of 2\n",
        "# - flatten layer\n",
        "# - two dense layers where the lats one has a softmax activation\n",
        "################################################################################\n",
        "\n",
        "...\n",
        "\n",
        "epochs = 20\n",
        "lrate = 0.001\n",
        "decay = lrate / epochs\n",
        "sgd = SGD(lr = lrate, momentum = 0.9, decay = decay, nesterov = False)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics = ['categorical_accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gafzgsZyjxBR"
      },
      "source": [
        "# Training phase\n",
        "hp_epochs = epochs\n",
        "hp_batch_size = 32\n",
        "hp_val_split = 0.2\n",
        "\n",
        "cnn_model = model.fit(x = train_features, \n",
        "                      y = train_labels,\n",
        "                      epochs = hp_epochs,\n",
        "                      batch_size = hp_batch_size,\n",
        "                      shuffle = True,\n",
        "                      validation_split = hp_val_split\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1Y4v63GcUgj"
      },
      "source": [
        "# Below we will plot loss across consecutive timepoints throughout the training\n",
        "\n",
        "#model.save_weights('its_cnn_weights.h5', overwrite = True)\n",
        "from google.colab import drive\n",
        "import keras\n",
        "\n",
        "#drive.mount('/content/drive')\n",
        "#model.save('/content/drive/its_cnn_model.h5')\n",
        "#model = keras.models.load_model('its_cnn_model.h5')\n",
        "loss = cnn_model.history['loss']\n",
        "val_loss = cnn_model.history['val_loss']\n",
        "epochs = range(hp_epochs)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6OmOW6sF7TT"
      },
      "source": [
        "# Below is the code to plot a confusion matrix as a heatmap.\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "\n",
        "predicted_labels = model.predict(np.stack(test_features))\n",
        "cm = confusion_matrix(np.argmax(test_labels, axis=1), \n",
        "                      np.argmax(predicted_labels, axis=1))\n",
        "print('Confusion matrix:\\n',cm)\n",
        "\n",
        "cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
        "\n",
        "plt.imshow(cm, cmap=plt.cm.Accent)\n",
        "plt.title('Normalized confusion matrix')\n",
        "plt.colorbar()\n",
        "plt.xlabel('Actual class')\n",
        "plt.ylabel('Predicted class')\n",
        "plt.xticks([0, 1]); plt.yticks([0, 1])\n",
        "plt.grid('off')\n",
        "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    plt.text(j, i, format(cm[i, j], '.2f'),\n",
        "             horizontalalignment='center',\n",
        "             color='white' if cm[i, j] > 0.5 else 'black')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzsbuZ7YGTSM"
      },
      "source": [
        "# Now, we evaluate our model on test data\n",
        "scores = model.evaluate(test_features, test_labels, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1] * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL-93J0OGhqT"
      },
      "source": [
        "# Code below will yield a saliency map for classifying the first 10 sequences.\n",
        "\n",
        "import keras.backend as K\n",
        "\n",
        "def compute_salient_bases(model, x):\n",
        "    input_tensors = [model.input]\n",
        "    gradients = model.optimizer.get_gradients(model.output[0][1], model.input)\n",
        "    compute_gradients = K.function(inputs = input_tensors, outputs = gradients)\n",
        "    \n",
        "    x_value = np.expand_dims(x, axis=0)\n",
        "    gradients = compute_gradients([x_value])[0][0]\n",
        "    sal = np.clip(np.sum(np.multiply(gradients,x), axis=1),a_min=0, a_max=None)\n",
        "    return sal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3s-7vEZVGjAl"
      },
      "source": [
        "K.set_learning_phase(1) #set learning phase\n",
        "sals = []\n",
        "for sequence_index in range(0,10):\n",
        "  sals.append(compute_salient_bases(model, input_features[sequence_index]))\n",
        "\n",
        "sal = sals[0]\n",
        "#print(sals)\n",
        "\n",
        "plt.figure(figsize=[16,5])\n",
        "barlist = plt.bar(np.arange(len(sal)), sal)\n",
        "[barlist[i].set_color('C1') for i in range(0,len(sal))]\n",
        "plt.xlabel('Bases')\n",
        "plt.ylabel('Saliency')\n",
        "plt.title('Saliency map');\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions\n",
        "\n",
        "Besides the code completion tasks defined in the comments above, there are a couple of additional questions/tasks:\n",
        "\n",
        "1. Try to alter filter and kernel size in the first layers to some small values. Examine and briefly discuss how this affects prediction accuracy. \n",
        "\n",
        "2. Does the k-mer size have an effect on model's performance. Try 3 or more different k-mer sizes to try to answer this question. \n",
        "\n",
        "3. What do you think of the saliency map we generated, are there any immediate patterns emerging? Do you think such map advances our understanding of the mechanics of the model?\n",
        "\n",
        "BONUS (1p)\n",
        "\n",
        "1. Can you think of any different/better CNN architecture for this classification task? If so, try to implement and test it. Does it perform better or worse than the network in the lab?\n",
        "\n",
        "2. Try making k-mers more noisy (randomly add Ns or other ambiguous symbols). How dos this affect the training performance with one-hot encoding vs. probabilities of 4 bases encoding? "
      ],
      "metadata": {
        "id": "Hq2ovwQftpQ6"
      }
    }
  ]
}